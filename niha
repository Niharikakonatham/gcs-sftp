# This is a Cloud Function Python script.

import base64
import requests
import json, os, sqlalchemy
import google.auth
from google.auth.transport.requests import AuthorizedSession
from datetime import datetime
from typing import Any
from google.cloud import storage

storage_client = storage.Client()

date_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S:%f")[:-4]

IAM_SCOPE = 'https://www.googleapis.com/auth/iam'
OAUTH_TOKEN_URI = 'https://www.googleapis.com/oauth2/v4/token'

AUTH_SCOPE = "https://www.googleapis.com/auth/cloud-platform"
CREDENTIALS, _ = google.auth.default(scopes=[AUTH_SCOPE])


def splunk_message(info, load_id, bucket_name, file_name, message):
    log_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S:%f")[:-4]
    print(
        f"[{info}] [{log_time}] [LOAD_ID: {load_id}] [APP_NAME: EPP] [SOURCE: {bucket_name}] [FILE_NAME: {file_name}] [LOB: ENT] MESSAGE: {message}")


# Make a Airflow Call
def make_composer2_web_server_request(url: str, method: str = "GET", **kwargs: Any) -> google.auth.transport.Response:
    authed_session = AuthorizedSession(CREDENTIALS)

    # Set the default timeout, if missing
    if "timeout" not in kwargs:
        kwargs["timeout"] = 90

    return authed_session.request(method, url, **kwargs)


# Talk to postgresql and fetch the needed info
def sql_search(file_name, load_id, bucket_name):
    cmd = sqlalchemy.text(
        f'select config_id,app_name,cluster_type,file_name from gcdccif.config_cif where \'{file_name}\' '
        f'LIKE file_pattern order by config_id desc limit 1')
    cloud_sql_connection_name = os.environ.get('cloud_sql_connection_name',
                                               'Cloud SQL Connection Name Variable is not set. i.e '
                                               '<PROJECT-NAME>:<INSTANCE-REGION>:<INSTANCE-NAME>')
    pool = sqlalchemy.create_engine(
        sqlalchemy.engine.url.URL(
            drivername="postgresql+pg8000",  # driver for postgres
            username=os.environ.get('pguser', 'pguser Variable is not set. i.e cif-env-user'),
            # e.g. "my-database-user"
            password="cifuser",  # e.g. "my-database-password"
            database=os.environ.get('pgdatabase', 'pgdatabase Variable is not set. i.e cif_env'),
            # e.g. "my-database-name"
            query={
                "unix_sock": f"/cloudsql/{cloud_sql_connection_name}/.s.PGSQL.5432"
            }
        ),  # db configs
        pool_size=5,  # Pool size is the maximum number of permanent connections to keep.
        max_overflow=5,  # Temporarily exceeds the set pool_size if no connections are available.
        pool_timeout=30,  # 'pool_timeout' is the maximum number of seconds to wait when retrieving a new connection
        # from the pool.
        pool_recycle=1800  # 'pool_recycle' is the maximum number of seconds a connection can persist.
    )
    try:  # connect to the above created pool and execute psql statement
        with pool.connect() as conn:
            output = conn.execute(cmd)
            for row in output:
                config_id = row[0]  # output is a dict
                app_name = row[1]
                cluster_type = row[2]
                file_name = row[3]
        if config_id != "" and app_name != "" and cluster_type != "" and file_name != "":
            return config_id, app_name, cluster_type, file_name
        else:
            message = "Configs not found"
            splunk_message("INFO", load_id, bucket_name, file_name, message)
            exit(1)
    except Exception as e:
        message = f"Exception block : {e.output}"
        splunk_message("INFO", load_id, bucket_name, file_name, message)
        return 'failed'


# Entry point of cloud function
def trigger_dag(event, context):
    load_id = datetime.now().strftime("%Y%m%d%H%M%S%f")[:-4]

    pubs_message = base64.b64decode(event['data']).decode('utf-8')
    pubs_message_dict = json.loads(pubs_message)
    bucket_name = pubs_message_dict["bucket"]
    pubs_file_name = pubs_message_dict["name"]
    file_name = pubs_file_name.split('/')[-1]
    if file_name.find('.json') == -1:
        psql_output = sql_search(file_name, load_id, bucket_name)

        config_id = psql_output[0]
        app_name = psql_output[1].lower()
        cluster_type = psql_output[2].lower()
        file_name_config = psql_output[3].replace("_", "-").lower()
        unique_num = datetime.now().strftime("%f")[:3]
        cluster_name_build = app_name.replace("_", "-") + "-" + file_name_config + "-" + unique_num
        if config_id != "failed":
            requested_url = os.environ.get('requested_url', 'Composer url Variable is not set.')
            load_id = date_time + "_" + unique_num
            config_string = {"conf": {"cluster_name": cluster_name_build,
                                      "load_id": load_id,
                                      "config_id": config_id,
                                      "cluster_size": cluster_type,
                                      "file_name": file_name}}
            response = make_composer2_web_server_request(requested_url, method="POST", json=config_string)
            if response.status_code == 403:
                raise requests.HTTPError(
                    "You do not have a permission to perform this operation. "
                    "Check Airflow RBAC roles for your account."
                    f"{response.headers} / {response.text}"
                )
            elif response.status_code != 200:
                response.raise_for_status()
                message = f"The requested Dag URL is {requested_url}"
                splunk_message("INFO", load_id, bucket_name, file_name, message)
            else:
                return response.text
        else:
            message = "Successfully extracted Information from PSQL"
            splunk_message("INFO", load_id, bucket_name, file_name, message)
    else:
        message = "Ignoring the Control file"
        splunk_message("INFO", load_id, bucket_name, file_name, message)
        
        
----------------------------------------------------------------------------------------------------


requests_toolbelt==0.9.1
SQLAlchemy==1.3.12
pg8000==1.13.2
google-cloud-storage==1.42.2
google-auth==2.3.3
requests==2.26.0
